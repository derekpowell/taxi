{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../scratch/wgerych/.cache/huggingface\n",
      "../../../../scratch/wgerych/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/healthy-ml/scratch/wgerych/py_envs/llm_conf/lib/python3.9/site-packages/timm/models/hub.py:4: DeprecationWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
      "/data/healthy-ml/scratch/wgerych/py_envs/llm_conf/lib/python3.9/site-packages/timm/models/layers/__init__.py:49: DeprecationWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", DeprecationWarning)\n",
      "/data/healthy-ml/scratch/wgerych/py_envs/llm_conf/lib/python3.9/site-packages/timm/models/registry.py:4: DeprecationWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('/scratch/wgerych'):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/scratch/wgerych/.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = '/scratch/wgerych/.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from easyeditor.util import nethook\n",
    "from easyeditor.custom import * # gets my custom functions\n",
    "\n",
    "# from easyeditor.editors import LOG\n",
    "# import logging\n",
    "# LOG.setLevel(logging.ERROR) # stops cluttering up notebook\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multichoice_dist(model, prompt, choices, normalization = None):\n",
    "\n",
    "    # prompt = prompt.rstrip() # remove any trailing whitespace\n",
    "\n",
    "    if type(model.tok) == transformers.models.llama.tokenization_llama.LlamaTokenizer:\n",
    "        padded_choices = choices\n",
    "        prompt = prompt + \" \" if prompt[-1]!= \" \" else prompt\n",
    "    else:\n",
    "        padded_choices = [pad_token(c) for c in choices] # pad all the \n",
    "    \n",
    "    prompts = [prompt + c for c in padded_choices]\n",
    "\n",
    "    logits = torch.tensor([model.completion_logprob(prompts[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "\n",
    "    if normalization == \"unconditional\":\n",
    "        norm_logits = torch.tensor([model.completion_logprob(padded_choices[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "        logits = logits - norm_logits\n",
    "\n",
    "    elif normalization == \"byte_length\":    \n",
    "        str_lens = [len(c) for c in choices]\n",
    "        logits = logits / torch.tensor(str_lens)\n",
    "\n",
    "    elif normalization == \"token_length\":\n",
    "        tok_lens = [len(encode_token(c, model.tok)) for c in choices]\n",
    "        logits = logits / torch.tensor(tok_lens)\n",
    "\n",
    "    elif normalization == \"root\":\n",
    "        tok_lens = [len(encode_token(c, model.tok)) for c in choices]\n",
    "        logits = torch.pow(torch.exp(logits), 1./torch.tensor(tok_lens))\n",
    "\n",
    "    logits = logits.tolist()\n",
    "\n",
    "    return(logits)\n",
    "    \n",
    "def compute_entropy(dist):\n",
    "    entropy = 0\n",
    "    for p in dist:\n",
    "        entropy -= p*np.log(p) \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def evaluate_with_uncertainty(evaluation_data, model, prefix_fwd = \"\", prefix_rev = \"\", normalization = None):\n",
    "\n",
    "    fwd_answers = []\n",
    "    rev_answers = []\n",
    "    fwd_dist = []\n",
    "    rev_dist = []\n",
    "    fwd_entropy = []\n",
    "    rev_entropy = []\n",
    "    corr_fwd_answers = []\n",
    "    corr_rev_answers = []\n",
    "\n",
    "    for q in evaluation_data.itertuples():\n",
    "\n",
    "        fwd_choices =  q.fwd_choices\n",
    "        query_fwd = q.query_fwd.replace(\"<subj>\", q.subj).replace(\"<answer>\", \"\")\n",
    "        if q.property not in [\"category_membership\", \"category_membership1\", \"category_membership2\",\"category_membership3\"]: # do not use prefix for these\n",
    "            query_fwd = prefix_fwd + query_fwd\n",
    "        # ans_fwd = model.choose(query_fwd, fwd_choices, normalization = normalization) # None, \"unconditional\", \"byte_length\", \"token_length\", \"root\"\n",
    "        mc_logits = get_multichoice_dist(model, query_fwd, fwd_choices, normalization = normalization)\n",
    "        ans_fwd = mc_logits.index(max(mc_logits))\n",
    "        mc_dist = np.exp(mc_logits) / np.sum(np.exp(mc_logits), axis=0)\n",
    "        entropy_fwd = compute_entropy(mc_dist)\n",
    "        fwd_entropy.append(entropy_fwd)\n",
    "        fwd_dist.append(mc_dist)\n",
    "\n",
    "\n",
    "        corr_fwd_answers.append(fwd_choices.index(q.answer_fwd))\n",
    "        fwd_answers.append(ans_fwd)\n",
    "\n",
    "        rev_choices =  q.rev_choices\n",
    "        query_rev = q.query_rev.replace(\"<answer>\", q.answer_fwd).replace(\"<subj>\", \"\")\n",
    "        if q.property not in [\"category_membership\", \"category_membership1\", \"category_membership2\",\"category_membership3\"]: # do not use prefix for these\n",
    "            query_rev = prefix_rev + query_rev\n",
    "        # ans_rev = model.choose(query_rev, rev_choices, normalization = normalization) # None, \"unconditional\", \"byte_length\", \"token_length\", \"root\"\n",
    "\n",
    "        mc_logits = get_multichoice_dist(model, query_rev, rev_choices, normalization = normalization)\n",
    "        ans_rev = mc_logits.index(max(mc_logits))\n",
    "        mc_dist = np.exp(mc_logits) / np.sum(np.exp(mc_logits), axis=0)\n",
    "        entropy_rev = compute_entropy(mc_dist)\n",
    "        rev_entropy.append(entropy_rev)\n",
    "        rev_dist.append(mc_dist)\n",
    "\n",
    "        corr_rev_answers.append(rev_choices.index(q.subj))\n",
    "        rev_answers.append(ans_rev)\n",
    "\n",
    "    results = (\n",
    "        evaluation_data\n",
    "        .assign(\n",
    "            corr_fwd_answer = corr_fwd_answers,\n",
    "            corr_rev_answer = corr_rev_answers,\n",
    "            fwd_predicted = fwd_answers,\n",
    "            rev_predicted = rev_answers,\n",
    "            fwd_dist = fwd_dist,\n",
    "            rev_dist = rev_dist,\n",
    "            fwd_entropy = fwd_entropy,\n",
    "            rev_entropy = rev_entropy\n",
    "            )\n",
    "        .assign(\n",
    "            correct_fwd = lambda x: x.corr_fwd_answer==x.fwd_predicted,\n",
    "            correct_rev = lambda x: x.corr_rev_answer==x.rev_predicted\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return(results)\n",
    "\n",
    "\n",
    "def edit_and_evaluate_with_uncertainty(edits_df, eval_df, model, edit_method, metrics = False, log_file = None, **kwargs):\n",
    "    \n",
    "    full_results = pd.DataFrame()\n",
    "    full_metrics = []\n",
    "\n",
    "    for e in edits_df.itertuples():\n",
    "        if e.edit_type == \"category membership\":\n",
    "            if edit_method in [\"ROME\", \"FT\", \"PMET\", \"GRACE\"]:\n",
    "                rewrite = {\n",
    "                        'prompts': [f'A {e.subj} is a kind of'],\n",
    "                        'target_new': [e.entity], #{'str': e.entity},\n",
    "                        'subject': [e.subj]\n",
    "                        }\n",
    "                metrics = model.edit(rewrite, log_file  = log_file)\n",
    "                full_metrics.append(metrics)\n",
    "            elif edit_method == \"ICE\":\n",
    "                model.edit({\"preprompt\": f\"Imagine that a {e.subj} is a kind of {e.entity} ...\\n\\n\"}) # and not a kind of {e.orig_entity}\n",
    "            \n",
    "            evals = eval_df.loc[lambda x: (x.edit_type == \"category membership\") & (x.entity == e.entity) & (x.subj == e.subj)]\n",
    "\n",
    "        elif e.edit_type == \"category property\":\n",
    "            if edit_method in [\"ROME\", \"FT\", \"PMET\", \"GRACE\"]:\n",
    "                rewrite_prompt = e.query_fwd.replace(\"<subj>\", e.entity).replace(\" <answer>\", \"\")\n",
    "                rewrite = {\n",
    "                    'prompts': [rewrite_prompt],\n",
    "                    'target_new': [e.answer_fwd], #{'str': e.entity},\n",
    "                    'subject': [e.entity]\n",
    "                }\n",
    "                metrics = model.edit(rewrite, log_file  = log_file)\n",
    "                full_metrics.append(metrics)\n",
    "\n",
    "            elif edit_method == \"ICE\":\n",
    "                \n",
    "                rewrite_prompt = e.query_fwd.replace(\"<subj>\", e.entity).replace(\"<answer>\", e.answer_fwd)\n",
    "                model.edit({\"preprompt\": f\"Imagine that {rewrite_prompt} ...\\n\\n\"}) # and not a kind of {e.orig_entity}    \n",
    "\n",
    "            evals = eval_df.loc[lambda x: (x.edit_type == \"category property\") & (x.entity == e.entity) & (x.property == e.property)]\n",
    "        \n",
    "        res = evaluate_with_uncertainty(evals, model, **kwargs)\n",
    "        \n",
    "        model.restore()\n",
    "\n",
    "        full_results = pd.concat([full_results, res])\n",
    "\n",
    "    full_results[\"edit_method\"] = edit_method\n",
    "    \n",
    "    return(full_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LLM = 'gpt2-xl'\n",
    "edit_methods = ['ICE'] #['ICE', 'FT', 'ROME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- set up test mode (or not)\n",
    "MODE_ARGS = [\"catmem_only\"] # []\n",
    "\n",
    "## --- load data\n",
    "\n",
    "def load_result(filename):\n",
    "    x = pd.read_csv(filename, converters={'fwd_choices':literal_eval, 'rev_choices':literal_eval})\n",
    "    return(x)\n",
    "\n",
    "baseline_df, edits_df, eval_df = load_data()\n",
    "\n",
    "prefix_fwd, prefix_rev, prefix_single = load_prefixes(verbose = False)\n",
    "\n",
    "# baseline_df =  baseline_df.loc[lambda x: (x.token_type == \"entity\") | (x.property == \"category_membership\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====== category membership edits only! =======\n"
     ]
    }
   ],
   "source": [
    "if \"catprop_only\" in MODE_ARGS:\n",
    "    print(\"====== category property edits only ! ======\")\n",
    "    edits_df = edits_df.loc[lambda x: x.edit_type == \"category property\"]\n",
    "    eval_df = eval_df.loc[lambda x: x.edit_type == \"category property\"]\n",
    "\n",
    "elif \"catmem_only\" in MODE_ARGS:\n",
    "    print(\" ====== category membership edits only! =======\")\n",
    "    edits_df = edits_df.loc[lambda x: x.edit_type == \"category membership\"]\n",
    "    eval_df = eval_df.loc[lambda x: x.edit_type == \"category membership\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_config = dict()\n",
    "results = dict()\n",
    "edit_hyperparams = {}\n",
    "edit_hyperparams['ICE'] = ROMEHyperParams\n",
    "edit_hyperparams['FT'] = FTHyperParams\n",
    "edit_hyperparams['ROME'] = ROMEHyperParams\n",
    "\n",
    "for edit_method in edit_methods:\n",
    "    hparam_config[edit_method] = {\"HyperParams\": edit_hyperparams[edit_method], \"path\": f'hparams/{edit_method}/{BASE_LLM}.yaml', \"edit_method\": edit_method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 17:36:42,718 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "02/13/2024 17:36:42 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "2024-02-13 17:36:56,299 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "02/13/2024 17:36:56 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "hparams = FTHyperParams.from_hparams(f'hparams/FT/{BASE_LLM}.yaml')\n",
    "base_model = EditedModel(hparams, auth_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline_eval = evaluate_with_uncertainty(eval_df, base_model, prefix_fwd = \"\", prefix_rev = \"\", normalization = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['fwd_entropy_baseline'] = results_baseline_eval['fwd_entropy']\n",
    "eval_df['rev_entropy_baseline'] = results_baseline_eval['rev_entropy']\n",
    "eval_df['fwd_dist_baseline'] = results_baseline_eval['fwd_dist']\n",
    "eval_df['rev_dist_baseline'] = results_baseline_eval['rev_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 17:51:47,122 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-02-13 17:51:47,122 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-02-13 17:51:47,122 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-02-13 17:51:47,122 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "02/13/2024 17:51:47 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "2024-02-13 17:51:59,491 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "2024-02-13 17:51:59,491 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "2024-02-13 17:51:59,491 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "2024-02-13 17:51:59,491 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "02/13/2024 17:51:59 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "for edit_method, HPARAMS in hparam_config.items():   \n",
    "        \n",
    "    edited_model = EditedModel(hparams, auth_token())\n",
    "\n",
    "    res = edit_and_evaluate_with_uncertainty(\n",
    "        edits_df[:1], \n",
    "        EVAL_DF, \n",
    "        edited_model, \n",
    "        edit_method, \n",
    "        prefix_fwd = \"\", \n",
    "        prefix_rev = \"\", \n",
    "        log_file = \"results/log-catmem-2024-02-12-b.txt\"\n",
    "        )\n",
    "\n",
    "    res.to_csv(\"results/csv/\" + hparams.model_name.replace(\"/\", \"-\") + \"-\" + edit_method +  \"catmem-full_w_uncertainty.csv\", index=False)\n",
    "    \n",
    "    results[HPARAMS[\"edit_method\"]] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
